---
title: "Practical Machine Learning Course Project"
author: "Ricardo Pommer"
date: "December 15, 2015"
output: html_document
---

The first and perhaps most important portion of this project was cleaning the dataset. The data was collected using a moving window of varying time-length (between .5 and 2 seconds). Several variables calculated statistical parameters for each window like skewness, standard deviation. Therefore each window had variables that were not continuous and instead took on a single value. These were manually removed by observation and preforming a column bind of all columns that did not have empty values (these were neither NA nor NULL, which complicated automation).

Similarly, identifier variables were discarded since we wanted to focus on numeric variables that could, for any window, any participant and at any point of the exercise, correctly predict the kind of exercise. It was particularly important to perform this imputation since the classe was ordered, making the index variable a very strong predictor, but one which would only work in the training data set. This eliminated columns one through seven.


```{r echo=FALSE}
library(caret)
library(randomForest)
data<-read.csv(file="pml-training.csv")
inTrain<-createDataPartition(y=data$classe,p=0.60,list=FALSE)
training<-data[inTrain,]
testing<-data[-inTrain,]
validation<-read.csv(file="pml-testing.csv")

training<-training[,8:160]
training<-training[,colSums(is.na(training))==0]
training1<-cbind(training[,1:4],training[14:35],training[42:44],training[54:66],training[76:86])

testing<-testing[,8:160]
testing<-testing[,colSums(is.na(testing))==0]
testing1<-cbind(testing[,1:4],testing[14:35],testing[42:44],testing[54:66],testing[76:86])

colnames(training1)

```

I then divided the data into training and testing partitions in a 60:40 proportion.
```{r}

set.seed(1990)

inTrain<-createDataPartition(y=data$classe,p=0.60,list=FALSE)
training<-data[inTrain,]
testing<-data[-inTrain,]
```


After having cleaned up the data, we used the randomForest function to fit the model. I was also careful to train without the variable "classe", since that is the one we are interested in predicting.

```{r cache=TRUE}
modFit<-randomForest(y=training1$classe,x=training1[,-53])
modFit
```

By using the predict function, I produced a measure of model accuracy for the testing portion of the dataset. The results were quite satisfactory, wih an in-sample error rate of 0.32% and an estimated out of sample error rate of 0.61%


```{r cache=TRUE}

pred <- predict(modFit,testing1[,-53])
testing1$predRight <- pred==testing1$classe
table(pred,testing1$classe)

```

We can expect the accuracy in the validation data to drop, which we can estimate to as p^20 if p is my probability of correctly predicting a case (out-of-sample), which we found to be 99.39%, then p^20 equal to 88.48%. 

Finally, we perform a prediction using the supplied testing cases in the file "pml-testing.csv"

```{r}

validation<-validation[,8:160]
validation<-validation[,colSums(is.na(validation))==0]
pred <- predict(modFit,validation[,-53])
pred

```

After having cleaned up the data, we used the randomForest function to fit the model.
```{r}
modFit<-randomForest(y=training1$classe,x=training1)
```

By using the predict function, I produced a measure of model accuracy for the testing portion of the dataset. The results were quite satisfactory, correctly predicting 100% of the cases.


```{r}

pred <- predict(modFit,testing1)
testing1$predRight <- pred==testing1$classe
table(pred,testing1$classe)

```


